{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangbinfeng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/latest.ckpt\n",
      "batch 0 ,test_accuracy 0.991\n",
      "batch 1 ,test_accuracy 0.988\n",
      "batch 2 ,test_accuracy 0.989\n",
      "batch 3 ,test_accuracy 0.993\n",
      "batch 4 ,test_accuracy 0.989\n",
      "average_accuracy 0.989999997616\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from load_data import *\n",
    "\n",
    "train_datas, train_labels=load_train_data()\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_datas, train_labels, test_size=0.1)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 32\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
    "    #W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长\n",
    "    #padding: A `string` from: `\"SAME\", \"VALID\"`\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize [1,x,y,1]\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "#定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,96,96,3])#96*96\n",
    "y = tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "\n",
    "#初始化第一个卷积层的权值和偏置\n",
    "W_conv1 = weight_variable([3,3,3,32])#3*3的采样窗口，3个卷积核从3个平面抽取特征\n",
    "b_conv1 = bias_variable([32])#每一个卷积核一个偏置值\n",
    "\n",
    "#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x,W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)#进行max-pooling\n",
    "\n",
    "#初始化第二个卷积层的权值和偏置\n",
    "W_conv2 = weight_variable([3,3,32,64])#3*3的采样窗口，64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([64])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)#进行max-pooling\n",
    "\n",
    "#初始化第三个卷积层的权值和偏置\n",
    "W_conv3 = weight_variable([3,3,64,128])#5*5的采样窗口，128个卷积核从64个平面抽取特征\n",
    "b_conv3 = bias_variable([128])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2,W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)#进行max-pooling\n",
    "\n",
    "#初始化第四个卷积层的权值和偏置\n",
    "W_conv4 = weight_variable([3,3,128,256])#5*5的采样窗口，256个卷积核从128个平面抽取特征\n",
    "b_conv4 = bias_variable([256])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv4 = tf.nn.relu(conv2d(h_pool3,W_conv4) + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)#进行max-pooling\n",
    "\n",
    "#初始化第五个卷积层的权值和偏置\n",
    "W_conv5 = weight_variable([3,3,256,256])#3*3的采样窗口，256个卷积核从256个平面抽取特征\n",
    "b_conv5 = bias_variable([256])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv5 = tf.nn.relu(conv2d(h_pool4,W_conv5) + b_conv5)\n",
    "h_pool5 = max_pool_2x2(h_conv5)#进行max-pooling\n",
    "\n",
    "#初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([3*3*256,1024])#上一层有3*3*256个神经元，全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([1024])#1024个节点\n",
    "\n",
    "#把池化层5的输出扁平化为1维\n",
    "h_pool5_flat = tf.reshape(h_pool5,[-1,3*3*256])\n",
    "#求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool5_flat,W_fc1) + b_fc1)\n",
    "\n",
    "#keep_prob用来表示神经元的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "#初始化第二个全连接层\n",
    "W_fc2 = weight_variable([1024,2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "#计算输出\n",
    "y_=tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "prediction = tf.nn.softmax(y_)\n",
    "\n",
    "#交叉熵代价函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_))\n",
    "#使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "#结果存放在一个布尔列表中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))#argmax返回一维张量中最大的值所在的位置\n",
    "\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    is_train=False\n",
    "    saver=tf.train.Saver(max_to_keep=1)\n",
    "    if is_train:\n",
    "        for n in range(len(x_train) //batch_size +1):\n",
    "            batch_xs = x_train[n*batch_size:n*batch_size+batch_size]\n",
    "            batch_ys = y_train[n*batch_size:n*batch_size+batch_size]\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.5})\n",
    "            if n % 100==0:\n",
    "                loss=sess.run(cross_entropy,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "                Accuracy=sess.run(accuracy,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "                val_loss=sess.run(cross_entropy,feed_dict={x:x_valid,y:y_valid,keep_prob:1.0})\n",
    "                val_Accuracy=sess.run(accuracy,feed_dict={x:x_valid,y:y_valid,keep_prob:1.0})\n",
    "                print(\"batch \"+str(n)+\" ,train_loss \"+\\\n",
    "                          \"{:.5f}\".format(loss)+\" ,training accuracy \"+str(Accuracy))\n",
    "                print(\"batch \"+str(n)+\" ,val_loss \"+\\\n",
    "                          \"{:.5f}\".format(val_loss)+\" ,val_Accuracy \"+str(val_Accuracy))\n",
    "                saver.save(sess,'ckpt/latest.ckpt')\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        model_file=tf.train.latest_checkpoint('ckpt/')\n",
    "        saver.restore(sess,model_file)\n",
    "        test_datas, test_labels=load_test_data()\n",
    "        average_accuracy=[]\n",
    "        batch_size=1000\n",
    "        for n in range(len(test_datas) //batch_size ):\n",
    "            batch_xs = test_datas[n*batch_size:n*batch_size+batch_size]\n",
    "            batch_ys = test_labels[n*batch_size:n*batch_size+batch_size]      \n",
    "            Accuracy=sess.run(accuracy,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "            average_accuracy.append(Accuracy)\n",
    "            print(\"batch \"+str(n)+\" ,test_accuracy \"+str(Accuracy))    \n",
    "        average_accuracy= sum(average_accuracy)/len(average_accuracy)\n",
    "        print(\"average_accuracy \"+str(average_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
