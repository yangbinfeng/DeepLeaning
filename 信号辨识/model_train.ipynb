{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from load_data import *\n",
    "\n",
    "train_datas, train_labels=load_train_data()\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_datas, train_labels, test_size=0.1)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size = 32\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
    "    #W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长\n",
    "    #padding: A `string` from: `\"SAME\", \"VALID\"`\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize [1,x,y,1]\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "#定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,96,96,3])#96*96\n",
    "y = tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "\n",
    "#初始化第一个卷积层的权值和偏置\n",
    "W_conv1 = weight_variable([3,3,3,32])#3*3的采样窗口，3个卷积核从3个平面抽取特征\n",
    "b_conv1 = bias_variable([32])#每一个卷积核一个偏置值\n",
    "\n",
    "#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x,W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)#进行max-pooling\n",
    "\n",
    "#初始化第二个卷积层的权值和偏置\n",
    "W_conv2 = weight_variable([3,3,32,64])#3*3的采样窗口，64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([64])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)#进行max-pooling\n",
    "\n",
    "#初始化第三个卷积层的权值和偏置\n",
    "W_conv3 = weight_variable([3,3,64,128])#5*5的采样窗口，128个卷积核从64个平面抽取特征\n",
    "b_conv3 = bias_variable([128])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2,W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)#进行max-pooling\n",
    "\n",
    "#初始化第四个卷积层的权值和偏置\n",
    "W_conv4 = weight_variable([3,3,128,256])#5*5的采样窗口，256个卷积核从128个平面抽取特征\n",
    "b_conv4 = bias_variable([256])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv4 = tf.nn.relu(conv2d(h_pool3,W_conv4) + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)#进行max-pooling\n",
    "\n",
    "#初始化第五个卷积层的权值和偏置\n",
    "W_conv5 = weight_variable([3,3,256,256])#3*3的采样窗口，256个卷积核从256个平面抽取特征\n",
    "b_conv5 = bias_variable([256])#每一个卷积核一个偏置值\n",
    "\n",
    "#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数\n",
    "h_conv5 = tf.nn.relu(conv2d(h_pool4,W_conv5) + b_conv5)\n",
    "h_pool5 = max_pool_2x2(h_conv5)#进行max-pooling\n",
    "\n",
    "#初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([3*3*256,1024])#上一层有3*3*256个神经元，全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([1024])#1024个节点\n",
    "\n",
    "#把池化层5的输出扁平化为1维\n",
    "h_pool5_flat = tf.reshape(h_pool5,[-1,3*3*256])\n",
    "#求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool5_flat,W_fc1) + b_fc1)\n",
    "\n",
    "#keep_prob用来表示神经元的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "#初始化第二个全连接层\n",
    "W_fc2 = weight_variable([1024,2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "#计算输出\n",
    "y_=tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "prediction = tf.nn.softmax(y_)\n",
    "\n",
    "#交叉熵代价函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_))\n",
    "#使用AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "#结果存放在一个布尔列表中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))#argmax返回一维张量中最大的值所在的位置\n",
    "\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    is_train=True\n",
    "    saver=tf.train.Saver(max_to_keep=1)\n",
    "    if is_train:\n",
    "        for n in range(len(x_train) //batch_size +1):\n",
    "            batch_xs = x_train[n*batch_size:n*batch_size+batch_size]\n",
    "            batch_ys = y_train[n*batch_size:n*batch_size+batch_size]\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.5})\n",
    "            if n % 100==0:\n",
    "                loss=sess.run(cross_entropy,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "                Accuracy=sess.run(accuracy,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "                val_loss=sess.run(cross_entropy,feed_dict={x:x_valid,y:y_valid,keep_prob:1.0})\n",
    "                val_Accuracy=sess.run(accuracy,feed_dict={x:x_valid,y:y_valid,keep_prob:1.0})\n",
    "                print(\"batch \"+str(n)+\" ,train_loss \"+\\\n",
    "                          \"{:.5f}\".format(loss)+\" ,training accuracy \"+str(Accuracy))\n",
    "                print(\"batch \"+str(n)+\" ,val_loss \"+\\\n",
    "                          \"{:.5f}\".format(val_loss)+\" ,val_Accuracy \"+str(val_Accuracy))\n",
    "                saver.save(sess,'ckpt/latest.ckpt')\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        model_file=tf.train.latest_checkpoint('ckpt/')\n",
    "        saver.restore(sess,model_file)\n",
    "        test_datas, test_labels=load_test_data()\n",
    "        average_accuracy=[]\n",
    "        batch_size=1000\n",
    "        for n in range(len(test_datas) //batch_size ):\n",
    "            batch_xs = test_datas[n*batch_size:n*batch_size+batch_size]\n",
    "            batch_ys = test_labels[n*batch_size:n*batch_size+batch_size]      \n",
    "            Accuracy=sess.run(accuracy,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "            average_accuracy.append(Accuracy)\n",
    "            print(\"batch \"+str(n)+\" ,test accuracy \"+str(Accuracy))    \n",
    "        average_accuracy= sum(average_accuracy)/len(average_accuracy)\n",
    "        print(\"average_accuracy \"+str(average_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch 0 ,train_loss 3865.73120 ,training accuracy 0.5\n",
    "batch 0 ,val_loss 4035.64062 ,val_Accuracy 0.494\n",
    "batch 100 ,train_loss 0.00000 ,training accuracy 1.0\n",
    "batch 100 ,val_loss 25.13863 ,val_Accuracy 0.966667\n",
    "batch 200 ,train_loss 0.00000 ,training accuracy 1.0\n",
    "batch 200 ,val_loss 12.48922 ,val_Accuracy 0.976\n",
    "batch 300 ,train_loss 0.00000 ,training accuracy 1.0\n",
    "batch 300 ,val_loss 8.71747 ,val_Accuracy 0.986\n",
    "batch 400 ,train_loss 0.00000 ,training accuracy 1.0\n",
    "batch 400 ,val_loss 4.31096 ,val_Accuracy 0.991333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面这个是不用模型，直接计算，训练集，测试集准确度100%\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "labels=[]\n",
    "labels=data['80'].values\n",
    "\n",
    "del data['80']\n",
    "correct_prediction=[]\n",
    "for i in range(data.shape[0]):\n",
    "    d = collections.Counter(data.iloc[i].values) #统计频次\n",
    "    t=np.array(list(tuple(d.values())))\n",
    "    if len(t)<80:\n",
    "        result=1\n",
    "    else:\n",
    "        result=0\n",
    "    correct_prediction.append(np.equal(result,labels[i]))\n",
    "accuracy=sum(correct_prediction)/len(correct_prediction)\n",
    "\n",
    "print('accuracy='+str(accuracy))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/yangbinfeng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.......\n",
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/1\n",
      "12000/12000 [==============================] - 3148s 262ms/step - loss: 0.5497 - acc: 0.9463 - val_loss: 0.0870 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.54967, saving model to weights.best.hdf5\n",
      "3000/3000 [==============================] - 865s 288ms/step\n",
      "scroe: 0.0869893558802 accuracy: 0.991333341599\n"
     ]
    }
   ],
   "source": [
    "#下面这个是用keras跑的。\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras import callbacks\n",
    "import numpy as np\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard,EarlyStopping\n",
    "from load_data import *\n",
    "\n",
    "images, labels=load_train_data()\n",
    "#images=np.load(\"images.npy\")\n",
    "#labels=np.load(\"labels.npy\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)\n",
    "base_model = VGG16(weights='imagenet', include_top=False,input_shape=(96, 96, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "#p=imagenet_utils.decode_predictions(predictions)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "       \n",
    "sgd = Adam(lr=0.0003)  \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])   #完成实际的模型配置工作\n",
    "print(\"train.......\")\n",
    "tensorboard = TensorBoard(log_dir='./logs',histogram_freq=1, write_graph=True, write_images=True)\n",
    "checkpoint = ModelCheckpoint(filepath=\"weights.best.hdf5\",monitor='loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, verbose=1)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,epochs = 1,verbose=1,shuffle=True,\n",
    "          validation_data=(x_test, y_test), callbacks=[tensorboard,checkpoint,early_stopping])\n",
    "scroe, accuracy = model.evaluate(x_test, y_test, batch_size=100)\n",
    "print('scroe:', scroe, 'accuracy:', accuracy) \n",
    "model.save(\"rec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.......\n",
    "Train on 12000 samples, validate on 3000 samples\n",
    "Epoch 1/1\n",
    "12000/12000 [==============================] - 3148s 262ms/step - loss: 0.5497 - acc: 0.9463 - val_loss: 0.0870 - val_acc: 0.9913\n",
    "\n",
    "Epoch 00001: loss improved from inf to 0.54967, saving model to weights.best.hdf5\n",
    "3000/3000 [==============================] - 865s 288ms/step\n",
    "scroe: 0.0869893558802 accuracy: 0.991333341599"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
